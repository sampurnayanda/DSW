{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10909cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression and multiple linear regression are two types of regression analysis techniques used to model \n",
    "the relationship between a dependent variable and one or more independent variables. Here are the key differences \n",
    "between \n",
    "the two: 1. Number of Independent Variables: - \n",
    "        Simple Linear Regression: In simple linear regression, there is only one \n",
    "            independent variable that is used to predict the dependent variable. The relationship between the \n",
    "            dependent \n",
    "            variable and the independent variable is modeled as a straight line. - \n",
    "        Multiple Linear Regression: In multiple linear regression, there are two or more independent variables \n",
    "            that are used to predict the dependent variable. The relationship between the dependent variable \n",
    "            and the independent variables is modeled as a linear combination of these variables. \n",
    "        2. Complexity of Model: - \n",
    "            Simple Linear Regression: Simple linear regression is a simpler model \n",
    "                compared to multiple linear regression as it involves only one independent variable. The model \n",
    "                equation is of the form Y = β0 + β1X, where Y is the dependent variable, X is the independent \n",
    "                variable, β0 is the intercept, and β1 is the coefficient. - \n",
    "            Multiple Linear Regression: Multiple linear regression is a more complex model as it involves \n",
    "                multiple independent variables. The model equation is of the form Y = β0 + β1X1 + β2X2 + ... + βnXn, \n",
    "                where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, β0 is the intercept, \n",
    "                and β1, β2, ..., βn are the coefficients for each independent variable.\n",
    "        3. Interpretation of Coefficients: - \n",
    "            Simple Linear Regression: In simple linear regression, there is only one coefficient to interpret, \n",
    "                which represents the relationship between the independent variable and the dependent variable. \n",
    "            - Multiple Linear Regression: In multiple linear regression, there are multiple coefficients to \n",
    "                interpret, each representing the relationship between a specific independent variable and the \n",
    "                dependent variable while holding other variables constant.\n",
    "        4. Assumptions: - Both simple linear regression and multiple linear regression rely on the same \n",
    "            assumptions, such as linearity, independence of errors, homoscedasticity, and normality of residuals.\n",
    "            However, multiple linear regression is more sensitive to multicollinearity, which occurs when independent \n",
    "            variables are highly correlated with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent: \n",
    "    1. Gradient descent is an optimization algorithm used to minimize a cost function by iteratively adjusting\n",
    "    the parameters of the model. \n",
    "    2. It is an iterative approach where the coefficients are updated in the \n",
    "    direction of the steepest descent of the cost function. \n",
    "    3. Gradient descent can be used for both linear and non-linear regression models.\n",
    "    4. It is computationally efficient for a large number of features or instances as it does not require \n",
    "    the inversion of a matrix. \n",
    "    5. Gradient descent may require hyperparameter tuning, such as learning rate,\n",
    "    to ensure convergence and avoid overshooting the minimum. \n",
    "Normal Equation Method: \n",
    "    1. The normal equation\n",
    "        method is a closed-form solution to calculate the coefficients of a linear regression model by directly\n",
    "        solving the normal equations. \n",
    "    2. It calculates the coefficients by finding the parameters that minimize the cost function analytically. \n",
    "    3. The normal equation method can provide the exact solution for regression coefficients without requiring iteration.\n",
    "    4. It can be computationally expensive for a large number of features or instances, as it involves the \n",
    "    inversion of a matrix (X^T*X), which can be computationally intensive for large datasets. \n",
    "    5. The normal equation method may not work well with highly correlated features or when the \n",
    "    matrix (X^T*X) is not invertible due to multicollinearity. \n",
    "    In summary, the main differences between gradient descent and the normal equation method for regression coefficients \n",
    "    estimation are the iterative vs. analytical nature of the methods, computational efficiency, and sensitivity to \n",
    "    hyperparameters and data characteristics. Gradient descent is more flexible and computationally efficient \n",
    "    for large datasets, while the normal equation method provides an exact solution but may be computationally \n",
    "    expensive for large datasets and sensitive to collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Assumption of Linearity: Linear regression assumes a linear relationship between the independent variables \n",
    "    and the dependent variable. If this assumption is violated, the model may not accurately capture the true \n",
    "    relationship in the data. \n",
    "    2. Multicollinearity: Multicollinearity occurs when independent variables in \n",
    "        the model are highly correlated with each other. This can lead to unstable coefficients and \n",
    "        make it challenging to interpret the individual effects of each variable on the dependent variable. \n",
    "        3. Heteroscedasticity: Heteroscedasticity refers to the unequal variance of errors across the range\n",
    "            of predictor variables. This violates the assumption of constant variance in linear regression,\n",
    "            leading to inaccurate parameter estimates and confidence intervals.\n",
    "        4. Outliers: Outliers are data points that do not follow the general trend of the data and \n",
    "                can significantly impact the results of a linear regression model. Outliers can influence\n",
    "                the estimated coefficients and reduce the model's predictive accuracy. \n",
    "        5. Overfitting or Underfitting: Overfitting occurs when the model captures noise in the\n",
    "                    training data rather than the underlying relationship, leading to poor generalization\n",
    "                    to new data. Underfitting, on the other hand, occurs when the model is too simple to\n",
    "                    capture the true relationship in the data, resulting in low predictive accuracy.\n",
    "        6. Non-linear Relationships: Linear regression assumes a linear relationship between\n",
    "                        the independent and dependent variables. If the true relationship is non-linear,\n",
    "                        linear regression may not be able to capture the complexity of the data accurately. \n",
    "        7. Independence of Errors: Linear regression assumes that the errors or residuals \n",
    "                            are independent of each other. Violation of this assumption can lead to biased \n",
    "                            parameter estimates and incorrect inferences. \n",
    "        8. Limited Expressive Power: Linear regression is a simple and interpretable \n",
    "                                model but may lack the expressive power to capture complex relationships \n",
    "                                in the data that can be captured by more flexible models like polynomial \n",
    "                                regression or decision trees. Addressing these issues involves careful \n",
    "                                data preprocessing, model selection, and diagnostics to ensure that the \n",
    "                                linear regression model is appropriate for the data and provides reliable\n",
    "                                results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd902fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent is an optimization algorithm that is commonly used to minimize the loss function in linear\n",
    "regression models. In the context of linear regression, the goal is to find the optimal values for the \n",
    "coefficients (weights) of the linear equation that best fit the data. \n",
    "Here's how Gradient Descent works in the context of linear regression: \n",
    "1. Initialize the coefficients: Start with some initial values for the coefficients of the linear equation. \n",
    "2. Calculate the gradient: Compute the gradient of the loss function with respect to each coefficient. \n",
    "    The gradient indicates the direction in which the coefficients should be adjusted to minimize the loss function. \n",
    "3. Update the coefficients: Adjust the coefficients in the opposite direction of the gradient to move towards\n",
    "    the minimum of the loss function. This is done iteratively using the following formula:\n",
    "        new_coefficient = old_coefficient - learning_rate * gradient Here, the learning rate is a \n",
    "        hyperparameter that controls the size of the steps taken during each iteration. \n",
    "4. Repeat steps 2 and 3: Continue calculating the gradient and updating the coefficients iteratively \n",
    "    until convergence is reached, i.e., when the changes in the coefficients become very small or \n",
    "    the loss function reaches a minimum. By iteratively updating the coefficients using the Gradient \n",
    "    Descent algorithm, the linear regression model is optimized to find the best-fitting line \n",
    "    that minimizes the error between the predicted values and the actual values in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bb1c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [2.08016218 2.76138544]\n",
      "Intercept: 240.5616715061651\n",
      "Predicted sales for new data points: [697.46750372]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data\n",
    "TV = np.array([127.44, 135.76, 130.14, 127.24, 121.18, 132.29, 121.88, 144.59, 148.18, 119.17]).reshape(-1, 1)\n",
    "Radio = np.array([66.95, 56.75, 68.38, 74.05, 56.22, 64.40, 64.80, 64.31, 55.58, 73.82]).reshape(-1, 1)\n",
    "Sales = np.array([716.54, 660.01, 658.56, 679.68, 632.94, 751.38, 691.32, 732.85, 691.15, 693.58])\n",
    "\n",
    "# Fit the multiple regression model\n",
    "X = np.hstack((TV, Radio))\n",
    "model = LinearRegression().fit(X, Sales)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "\n",
    "# Predict sales for new data points\n",
    "new_TV = np.array([140.0]).reshape(-1, 1)\n",
    "new_Radio = np.array([60.0]).reshape(-1, 1)\n",
    "new_X = np.hstack((new_TV, new_Radio))\n",
    "predicted_sales = model.predict(new_X)\n",
    "print(\"Predicted sales for new data points:\", predicted_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada88ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model Coefficients:\n",
      "Intercept: 37.463442069741276\n",
      "Coefficient: 1.0416197975253096\n",
      "\n",
      "Polynomial Regression Model Coefficients:\n",
      "Intercept: -103.8179600958095\n",
      "Coefficients: [ 0.00000000e+00  1.57135372e+01 -3.66067751e-01 -2.86182579e-04]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Given data\n",
    "X = np.array([15, 23, 18, 23, 24, 22, 22, 19, 19, 16]).reshape(-1, 1)\n",
    "Y = np.array([49, 63, 58, 60, 58, 61, 60, 63, 60, 52])\n",
    "\n",
    "# Linear regression model\n",
    "linear_model = LinearRegression().fit(X, Y)\n",
    "\n",
    "# Polynomial regression model of degree 3\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "poly_model = LinearRegression().fit(X_poly, Y)\n",
    "\n",
    "# Print coefficients of linear regression model\n",
    "print(\"Linear Regression Model Coefficients:\")\n",
    "print(\"Intercept:\", linear_model.intercept_)\n",
    "print(\"Coefficient:\", linear_model.coef_[0])\n",
    "\n",
    "# Print coefficients of polynomial regression model\n",
    "print(\"\\nPolynomial Regression Model Coefficients:\")\n",
    "print(\"Intercept:\", poly_model.intercept_)\n",
    "print(\"Coefficients:\", poly_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The normal equation method is a mathematical approach used to find the parameters (coefficients) of a multiple \n",
    "linear regression model directly, without requiring iterative optimization algorithms like gradient descent.\n",
    "It provides a closed-form solution for the optimal parameters that minimize the sum of squared errors between \n",
    "the predicted values and the actual values in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of the normal equation method:\n",
    "\n",
    "No need for feature scaling: Unlike gradient descent, the normal equation method does not require \n",
    "    feature scaling,\n",
    "    making it convenient when dealing with features on different scales.\n",
    "Direct solution: The normal equation provides a direct solution for the optimal parameters without \n",
    "    the need for iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0654a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters (coefficients) estimated by the normal equation method:\n",
      "[37.46344207  1.0416198 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normal_equation(X, y):\n",
    " \n",
    "    # Add intercept term to X if it's not already included\n",
    "    if X.shape[1] == 1:\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    # Compute parameters using the normal equation\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 15], [1, 23], [1, 18], [1, 23], [1, 24], [1, 22], [1, 22], [1, 19], [1, 19], [1, 16]])\n",
    "y = np.array([49, 63, 58, 60, 58, 61, 60, 63, 60, 52])\n",
    "theta = normal_equation(X, y)\n",
    "print(\"Parameters (coefficients) estimated by the normal equation method:\")\n",
    "print(theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
