{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1)In the context of logistic regression, \n",
    "the prediction function is the sigmoid function, which maps the input features to a probability between 0 and 1. \n",
    "The cost function used in logistic regression is the cross-entropy loss function, \n",
    "which measures the difference between the predicted probabilities and the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a6b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict(theta, X):\n",
    "    return sigmoid(np.dot(X, theta))\n",
    "\n",
    "def cost_function(theta, X, y):\n",
    "    m = len(y)\n",
    "    h = predict(theta, X)\n",
    "    cost = -1/m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2)There are several types of logistic regression models based on the number of categories in \n",
    "the target variable and the problem being solved. Some common types of logistic regression include: \n",
    "1. Binary Logistic Regression: This is the most common type of logistic regression where the target variable \n",
    "    has only two possible outcomes, typically coded as 0 and 1. It is used for binary classification problems. \n",
    "2. Multinomial Logistic Regression: In multinomial logistic regression, the target variable has three or more \n",
    "    categories that are not ordered. This type of logistic regression is used for multi-class classification \n",
    "    problems where the classes are not ordered. \n",
    "3. Ordinal Logistic Regression: In ordinal logistic regression, the target variable has three or more ordered categories. \n",
    "    This type of logistic regression is used when the target variable has a natural order or ranking. \n",
    "4. Regularized Logistic Regression: Regularized logistic regression includes regularization terms in the cost \n",
    "function to prevent overfitting. Common regularization techniques include L1 (Lasso) regularization and \n",
    "L2 (Ridge) regularization. \n",
    "5. Imbalanced Logistic Regression: Imbalanced logistic regression is used when the target variable in the \n",
    "    dataset is imbalanced, meaning that one class is much more prevalent than the other class. \n",
    "    Techniques such as oversampling, undersampling, or using class weights can be used to handle imbalanced datasets.\n",
    "    These are some of the common types of logistic regression models used in different machine learning and \n",
    "    statistical applications. The choice of which type of logistic regression to use depends on the nature of the \n",
    "    problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(3)Linear regression and logistic regression are two different types of regression models used in statistical analysis\n",
    "and machine learning. Here are some key differences between the two:\n",
    "    1. Target Variable: - Linear Regression: In linear regression, the target variable is continuous and can take \n",
    "            any real value. The goal is to predict a quantitative outcome.\n",
    "            - Logistic Regression: In logistic regression, the target variable is categorical and is used to predict \n",
    "                the probability of a binary outcome (e.g., yes/no, 0/1). \n",
    "    2. Output: - Linear Regression: The output of linear regression is a continuous value that represents the \n",
    "            predicted outcome. - Logistic Regression: The output of logistic regression is a probability value \n",
    "                between 0 and 1, which can be converted into class labels based on a threshold \n",
    "                (e.g., if p >= 0.5, class 1; else class 0). \n",
    "    3. Model Assumptions: - Linear Regression: Linear regression assumes a linear relationship between the independent\n",
    "            variables and the target variable. - Logistic Regression: Logistic regression does not assume a linear \n",
    "                relationship between the independent variables and the target variable. \n",
    "                Instead, it models the log-odds of the probability of the target variable. \n",
    "    4. Cost Function: - Linear Regression: The cost function in linear regression is based on the sum of squared errors \n",
    "            (SSE) or mean squared error (MSE). - Logistic Regression: The cost function in logistic regression is based\n",
    "                on the log-likelihood function, which is used to maximize the likelihood of observing the target variable\n",
    "                given the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6455ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters after 3 iterations: [-7.27555640e-05  7.39376323e-03  9.20007767e-03]\n",
      "Prediction for [1, 1.5] with optimized parameters: 0.505280084756206\n"
     ]
    }
   ],
   "source": [
    "(4)import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = np.array([[0.5, 1],\n",
    "              [1, 2],\n",
    "              [1.5, 2.5],\n",
    "              [2, 3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Preprocess the data\n",
    "m = len(y)\n",
    "X = np.hstack((np.ones((m, 1)), X))  # Add bias term\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# Gradient Descent parameters\n",
    "alpha = 0.01\n",
    "iterations = 3\n",
    "\n",
    "# Gradient Descent\n",
    "for i in range(iterations):\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    gradient = np.dot(X.T, (h - y)) / m\n",
    "    theta -= alpha * gradient\n",
    "\n",
    "# Optimized parameters after 3 iterations\n",
    "print(\"Optimized parameters after 3 iterations:\", theta)\n",
    "\n",
    "# Prediction for [1, 1.5] with optimized parameters\n",
    "new_X = np.array([1, 1.5])\n",
    "new_X = np.insert(new_X, 0, 1)  # Add bias term\n",
    "prediction = predict(theta, new_X)\n",
    "print(\"Prediction for [1, 1.5] with optimized parameters:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f18714",
   "metadata": {},
   "outputs": [],
   "source": [
    "(5)The K-Nearest Neighbors (KNN) algorithm is a simple yet powerful supervised machine learning algorithm used \n",
    "for both classification and regression tasks. It's a non-parametric and instance-based learning algorithm, \n",
    "meaning it doesn't make assumptions about the underlying data distribution and learns directly from the \n",
    "training instances themselves.\n",
    "\n",
    "Here's how the KNN algorithm works:\n",
    "\n",
    "Store Training Data: KNN stores all available cases and their class labels (for classification) or the output \n",
    "    values (for regression).\n",
    "Choose the Number of Neighbors (K): K is a hyperparameter that represents the number of nearest neighbors to consider \n",
    "    when making predictions. It's typically chosen based on experimentation and cross-validation.\n",
    "Calculate Distance: For a given unseen input instance, the algorithm calculates the distance between that instance \n",
    "    and all other instances in the training set. Common distance metrics include Euclidean distance, \n",
    "    Manhattan distance, and Minkowski distance.\n",
    "Find K Nearest Neighbors: It then identifies the K nearest neighbors based on the calculated distances. \n",
    "    These are the K instances with the smallest distances to the unseen instance.\n",
    "Majority Vote (Classification) / Mean (Regression): For classification tasks, the algorithm assigns \n",
    "    the class label that is most common among the K nearest neighbors. In other words, \n",
    "    it performs a majority vote among the neighbors. For regression tasks, it calculates \n",
    "    the mean of the output values of the K nearest neighbors.\n",
    "Make Prediction: Finally, it assigns the predicted class label or output value to the \n",
    "    unseen instance based on the majority vote (classification) or mean (regression) \n",
    "    calculated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65771f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "(6)Choosing the optimal value of k in a K-Nearest Neighbors (KNN) model is an important step in building an effective \n",
    "predictive model. Here are some common methods to determine the optimal k value: \n",
    "    1. Cross-Validation: One of the most common methods is to use cross-validation techniques such as k-fold cross-validation. \n",
    "        This involves splitting the training data into k subsets, training the model on k-1 subsets, and \n",
    "        validating it on the remaining subset. This process is repeated for different values of k, and the one that \n",
    "        gives the best performance metric (e.g., accuracy, F1 score) is selected. \n",
    "    2. Grid Search: Use grid search or \n",
    "            random search techniques to search through a range of k values and evaluate the model's performance on a validation\n",
    "            set. This allows you to systematically explore different k values and select the one that performs the best.\n",
    "    3. Elbow Method: Plot the performance metric (e.g., accuracy) of the KNN model for different values of k. \n",
    "            Look for the point where the performance metric stops decreasing significantly with increasing k. \n",
    "            This point is often referred to as the \"elbow point\" and can be a good indication of the optimal k value.\n",
    "    4. Domain Knowledge: Consider the nature of your data and the problem you are trying to solve. \n",
    "            Some datasets or problems may have an inherent optimal k value based on the underlying patterns in the data. \n",
    "            Domain knowledge can help guide the selection of k. \n",
    "    5. Experimentation: Lastly, it may be beneficial to \n",
    "                experiment with different values of k and evaluate the model's performance on a validation set.this trial and \n",
    "            error \n",
    "            approach can help you gain insights into how different k values affect the model's performance. \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a8c006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for [1.5, 1] with k=2: Class 0\n",
      "Prediction for [1.5, 1] with k=3: Class 0\n"
     ]
    }
   ],
   "source": [
    "(7)import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "X = np.array([[0.5, 0.5],\n",
    "              [0.5, 1],\n",
    "              [1, 1],\n",
    "              [2, 2.5],\n",
    "              [2.5, 3],\n",
    "              [3, 3]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Input to predict\n",
    "new_X = np.array([1.5, 1])\n",
    "\n",
    "# Function to calculate Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "# Function to predict class for a given input and k\n",
    "def predict_class(X_train, y_train, new_X, k):\n",
    "    distances = [euclidean_distance(new_X, x) for x in X_train]\n",
    "    nearest_indices = np.argsort(distances)[:k]\n",
    "    nearest_labels = y_train[nearest_indices]\n",
    "    unique_labels, counts = np.unique(nearest_labels, return_counts=True)\n",
    "    majority_label = unique_labels[np.argmax(counts)]\n",
    "    return majority_label\n",
    "\n",
    "# Predict class for k=2 and k=3\n",
    "k_values = [2, 3]\n",
    "for k in k_values:\n",
    "    prediction = predict_class(X, y, new_X, k)\n",
    "    print(f\"Prediction for [1.5, 1] with k={k}: Class {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e276505",
   "metadata": {},
   "outputs": [],
   "source": [
    "(8)Yes, if the dataset is imbalanced, the predictions made by the K-Nearest Neighbors (KNN) algorithm \n",
    "can be biased towards the majority class. This bias occurs because KNN relies on the majority class \n",
    "to determine the class of a new instance based on the nearest neighbors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
